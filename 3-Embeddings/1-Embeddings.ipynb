{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb73af29-1b57-4bef-ae0d-044922277180",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embeddings are integral to various natural language processing applications, with their quality crucial for optimal performance. They are commonly used in knowledge bases to represent textual data as dense vectors enabling efficient similarity search and retrieval. In Retrieval Augmented Generation (RAG), embeddings are used to retrieve relevant passages from a corpus to provide context for language models to generate informed, knowledge-grounded responses. Embeddings also play a key role in personalization and recommendation systems by representing user preferences, item characteristics, and historical interactions as vectors, allowing calculation of similarities for personalized recommendations based on user behavior and item embeddings. As new embedding models are released with incremental quality improvements, organizations must weigh the potential benefits against the associated costs of upgrading, considering factors like computational resources, data preprocessing, integration efforts, and projected performance gains impacting business metrics.\n",
    "\n",
    "#### How a piece of text is converted into a vector?\n",
    "Common approach is to use models which can provide contextualized embeddings for entire sentences. These models are based on deep learning architectures such as Transformers, which can capture the contextual information and relationships between words in a sentence more effectively.\n",
    "\n",
    "![Embedding Model](./images/vector_embedding.png)\n",
    "\n",
    "In addition to semantic search, you can use embeddings to augment your prompts for more accurate results through Retrieval Augmented Generation (RAG)‚Äîbut in order to use them, you‚Äôll need to store them in a database with vector capabilities.\n",
    "\n",
    "![Embedding Model](./images/vector_db.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347c523-adac-43c8-b651-acc6281afd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain_cohere -q\n",
    "#%pip install spacy -q\n",
    "#%pip install python-dotenv -q\n",
    "#ignore error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d4086-c974-496a-99e1-2d5584e0755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you need to run this in a terminal window\n",
    "# python -m spacy download en_core_web_md\n",
    "# now restart your kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a4fed-79a7-4616-96b5-813c6f5bcc9a",
   "metadata": {},
   "source": [
    "Standard imports for the libraires we will be using in this notebook.  Try to keep your imports in the first cell so this can this code can more easliy be converted into a python program later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1513731f-ab9f-4e74-9249-e8ff08dfd433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import traceback\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.embeddings import SpacyEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616dc79-989e-44d8-a1e4-8c5773ea0fb2",
   "metadata": {},
   "source": [
    "#### Boto3 client to connect to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a51a6e37-cbbb-4444-8f0c-a2f00c5836ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "def get_bedrock_client(assumed_role: Optional[str] = None, region: Optional[str] = 'us-west-2',runtime: Optional[bool] = True,external_id=None, ep_url=None):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides \n",
    "    \"\"\"\n",
    "    target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}:external_id={external_id}: \")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    # üîç Add this right after session is created\n",
    "    creds = session.get_credentials().get_frozen_credentials()\n",
    "    print(f\"‚úÖ Using Access Key: {creds.access_key[:4]}...{creds.access_key[-4:]}\")\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        if external_id:\n",
    "            response = sts.assume_role(\n",
    "                RoleArn=str(assumed_role),\n",
    "                RoleSessionName=\"langchain-llm-1\",\n",
    "                ExternalId=external_id\n",
    "            )\n",
    "        else:\n",
    "            response = sts.assume_role(\n",
    "                RoleArn=str(assumed_role),\n",
    "                RoleSessionName=\"langchain-llm-1\",\n",
    "            )\n",
    "        print(f\"Using role: {assumed_role} ... sts::successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name='bedrock-runtime'\n",
    "    else:\n",
    "        service_name='bedrock'\n",
    "\n",
    "    if ep_url:\n",
    "        bedrock_client = session.client(service_name=service_name,config=retry_config,endpoint_url = ep_url, **client_kwargs )\n",
    "    else:\n",
    "        bedrock_client = session.client(service_name=service_name,config=retry_config, **client_kwargs )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010a53f-8b7c-441e-9318-5404be8197b6",
   "metadata": {},
   "source": [
    "#### Helper Class to connect and run the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3def53b-7ff0-4f76-8d80-e980909d8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "class TitanEmbeddings(object):\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    \n",
    "    def __init__(self, model_id=\"amazon.titan-embed-text-v2:0\", boto3_client=None, region_name='us-west-2'):\n",
    "        \n",
    "        if boto3_client:\n",
    "            self.bedrock_boto3 = boto3_client\n",
    "        else:\n",
    "            # self.bedrock_boto3 = boto3.client(service_name='bedrock-runtime')\n",
    "            self.bedrock_boto3 = boto3.client(\n",
    "                service_name='bedrock-runtime', \n",
    "                region_name=region_name, \n",
    "            )\n",
    "        self.model_id = model_id\n",
    "\n",
    "    def __call__(self, text, dimensions, normalize=True):\n",
    "        \"\"\"\n",
    "        Returns Titan Embeddings\n",
    "\n",
    "        Args:\n",
    "            text (str): text to embed\n",
    "            dimensions (int): Number of output dimensions.\n",
    "            normalize (bool): Whether to return the normalized embedding or not.\n",
    "\n",
    "        Return:\n",
    "            List[float]: Embedding\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        body = json.dumps({\n",
    "            \"inputText\": text,\n",
    "            \"dimensions\": dimensions,\n",
    "            \"normalize\": normalize\n",
    "        })\n",
    "\n",
    "        response = self.bedrock_boto3.invoke_model(\n",
    "            body=body, modelId=self.model_id, accept=self.accept, contentType=self.content_type\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "\n",
    "        return response_body['embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be77df7f-db2b-42cd-8872-46f69708001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2:external_id=None: \n",
      "‚úÖ Using Access Key: AKIA...Q65M\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "aws_client = get_bedrock_client() #boto3.client('bedrock')\n",
    "\n",
    "bedrock_embeddings = TitanEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\", boto3_client=aws_client)\n",
    "bedrock_embeddings\n",
    "# Create the AWS client for the Bedrock runtime with boto3\n",
    "aws_client = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481e19c-0ab2-474f-9a67-653a2b8a0139",
   "metadata": {},
   "source": [
    "### Generate Embeddings\n",
    "\n",
    "At the time of writing you can use amazon.titan-embed-text-v2 as embedding model via the API. The input text size is 8k tokens and the output vector length can be any of 256, 512 or 1024\n",
    "\n",
    "To use a text embeddings model, use the InvokeModel API operation or the Python SDK. Use InvokeModel to retrieve the vector representation of the input text from the specified model.\n",
    "Input\n",
    "\n",
    "```\n",
    "\n",
    "{\n",
    "    \"inputText\": text,\n",
    "    \"dimensions\": dimensions, # range from 256 , 512, 1024\n",
    "    \"normalize\": normalize\n",
    "}\n",
    "\n",
    "Output\n",
    "\n",
    "{\n",
    "    \"embedding\": []\n",
    "}\n",
    "```\n",
    "\n",
    "#### Normalization of a vector \n",
    "\n",
    "Normalization is the process of scaling it to have a unit length or magnitude of 1. It is useful to ensure that all vectors have the same scale and contribute equally during vector operations, preventing some vectors from dominating others due to their larger magnitudes.\n",
    "\n",
    "#### When should you Normalize:\n",
    "Use this as default for most of the use cases like Retrieval, RAG and others\n",
    "\n",
    "#### When you should not Normalize: \n",
    "Normally normalization wil work for all use cases, but you can experiment for certain use cases like Classification or Entity extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "579e61be-17f2-4edd-81ea-10a5ac462cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding vector has 256 values\n",
      "[-0.1435401737689972, 0.06038207560777664, -0.05212177708745003, '...', 0.15111945569515228, 0.05338708311319351, 0.002391696209087968]\n"
     ]
    }
   ],
   "source": [
    "prompt_data = \"Amazon Bedrock supports foundation models from industry-leading providers such as \\\n",
    "AI21 Labs, Anthropic, Stability AI, and Amazon. Choose the model that is best suited to achieving \\\n",
    "your unique goals.\"\n",
    "\n",
    "\n",
    "modelId = \"amazon.titan-embed-text-v2:0\"  # \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "\n",
    "\n",
    "sample_model_input={\n",
    "    \"inputText\": prompt_data,\n",
    "    \"dimensions\": 256,\n",
    "    \"normalize\": True\n",
    "}\n",
    "\n",
    "body = json.dumps(sample_model_input)\n",
    "\n",
    "response = aws_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "embedding = response_body.get(\"embedding\")\n",
    "print(f\"The embedding vector has {len(embedding)} values\\n{embedding[0:3]+['...']+embedding[-3:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceec844-3eee-4ce4-993d-d72ab9dd8560",
   "metadata": {},
   "source": [
    "#### Lets define functions that will use various embedding models so we can generate vector embeddings\n",
    "Spacey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd1b6024-dda4-4dd3-836d-5830b472d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spacy_vector_embedding(text):\n",
    "    embedder = SpacyEmbeddings(model_name=\"en_core_web_md\")\n",
    "    query_embedding = embedder.embed_query(text)\n",
    "\n",
    "    return(np.array(query_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35157a12-af69-4d79-8a9f-d7229eef4bec",
   "metadata": {},
   "source": [
    "Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0667bf3-01ef-4493-8cbc-744413c0d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send in an array size of one and only return the 0th element\n",
    "def generate_cohere_vector_embedding(text_data):\n",
    "    input_type = \"clustering\"\n",
    "    truncate = \"NONE\" # optional\n",
    "    model_id = \"cohere.embed-english-v3\" # or \"cohere.embed-multilingual-v3\"\n",
    "    \n",
    "    # Create the JSON payload for the request\n",
    "    json_params = {\n",
    "            'texts': [text_data],\n",
    "            'truncate': truncate, \n",
    "            \"input_type\": input_type\n",
    "        }\n",
    "    json_body = json.dumps(json_params)\n",
    "    params = {'body': json_body, 'modelId': model_id,}\n",
    "    \n",
    "    # Invoke the model and print the response\n",
    "    result = aws_client.invoke_model(**params)\n",
    "    response = json.loads(result['body'].read().decode())\n",
    "    return(np.array(response['embeddings'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0f33d-429d-4636-9082-d7bbb9850790",
   "metadata": {},
   "source": [
    "Amazon Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5794765f-0b32-451e-a145-6a7b4ce785e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a dense vector using Amazon Titan with LangChain\n",
    "def generate_titan_vector_embedding(text):\n",
    "    #create an Amazon Titan Text Embeddings client\n",
    "    embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\") \n",
    "\n",
    "    #Invoke the model\n",
    "    embedding = embeddings_client.embed_query(text)\n",
    "    return(np.array(embedding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f08b3826-e751-4f03-b564-e2eb2d3ff00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a dense vector using Amazon Titan without using a np.array as a return value\n",
    "def generate_vector_embedding(text):\n",
    "    #create an Amazon Titan Text Embeddings client\n",
    "    embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\") \n",
    "\n",
    "    #Invoke the model\n",
    "    embedding = embeddings_client.embed_query(text)\n",
    "    #Note pgvector does not want a np.array as out manual method\n",
    "    return(embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff40e7e-c880-4dd1-b7ad-70f8a12efe2a",
   "metadata": {},
   "source": [
    "This is the mathmatical formula to calcuate cosine similarity between 2 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e8367a7-c786-41fc-ad05-d53b5633d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6763ad27-f4b8-4f6a-8200-d067332ea4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(value):\n",
    "    value_str = str(value)\n",
    "    cleaned_value = ''.join(char for char in value_str if char.isalnum() or char.isspace())\n",
    "    return cleaned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b14383a5-eec2-40e7-9c61-ccadb437d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_string_size(x, max_chars=2048):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(x, str):\n",
    "        return x[:max_chars]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7c74692-d25d-48ae-8fde-04ba53188a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_values(list_stuff: list, num_items: int) -> None:\n",
    "    i=0\n",
    "    for item in list_stuff:\n",
    "        i=i+1\n",
    "        if i>num_items:\n",
    "            return None\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d11db83-cffa-4ebc-8f41-5392bb92cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m1/tx6ck0f962jd1hq4ch5phg_80000gn/T/ipykernel_36750/1231035754.py:4: LangChainDeprecationWarning: The class `BedrockEmbeddings` was deprecated in LangChain 0.2.11 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-aws package and should be used instead. To use it run `pip install -U :class:`~langchain-aws` and import as `from :class:`~langchain_aws import BedrockEmbeddings``.\n",
      "  embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This embedding has 1536 dimensions\n",
      "[-0.25       -0.91015625  0.55078125  0.59375     0.5546875 ]\n"
     ]
    }
   ],
   "source": [
    "#Titan\n",
    "king_vector = generate_titan_vector_embedding(\"King\")\n",
    "queen_vector = generate_titan_vector_embedding(\"Queen\")\n",
    "man_vector = generate_titan_vector_embedding(\"man\")\n",
    "woman_vector = generate_titan_vector_embedding(\"woman\")\n",
    "print(f\"This embedding has {len(king_vector)} dimensions\")\n",
    "print(king_vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50f2dbfe-0035-4034-bc50-01b58462ae16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity distance between Titan calculated_queen - Queen: 0.6676\n",
      "Cosine Similarity of Titan King to Queen: 0.6291\n"
     ]
    }
   ],
   "source": [
    "calculated_queen_vector = king_vector - man_vector + woman_vector\n",
    "\n",
    "similarity = cosine_similarity(calculated_queen_vector, queen_vector)\n",
    "print(f\"Cosine Similarity distance between Titan calculated_queen - Queen: {similarity:.4f}\")\n",
    "\n",
    "similarity = cosine_similarity(king_vector, queen_vector)\n",
    "print(f\"Cosine Similarity of Titan King to Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faec8edc-da15-4f5b-8d5e-ad5af3576a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This embedding has 1024 dimensions\n",
      "[ 0.02383423 -0.0247345   0.02523804 -0.03985596 -0.06323242]\n"
     ]
    }
   ],
   "source": [
    "# Input cohere for embedding \n",
    "king_vector = generate_cohere_vector_embedding('King')\n",
    "queen_vector = generate_cohere_vector_embedding(\"Queen\")\n",
    "man_vector = generate_cohere_vector_embedding(\"man\")\n",
    "woman_vector = generate_cohere_vector_embedding(\"woman\")\n",
    "print(f\"This embedding has {len(king_vector)} dimensions\")\n",
    "print(king_vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b4a6dca-a476-468e-ac44-dbef8f363899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity distance between Cohere calculated_queen - Queen: 0.7225\n",
      "Cosine Similarity of Cohere King to Queen: 0.7343\n"
     ]
    }
   ],
   "source": [
    "calculated_queen_vector = king_vector - man_vector + woman_vector\n",
    "\n",
    "similarity = cosine_similarity(calculated_queen_vector, queen_vector)\n",
    "print(f\"Cosine Similarity distance between Cohere calculated_queen - Queen: {similarity:.4f}\")\n",
    "\n",
    "similarity = cosine_similarity(king_vector, queen_vector)\n",
    "print(f\"Cosine Similarity of Cohere King to Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5853152e-aee5-4ebd-9496-13310f6eab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This embedding has 300 dimensions\n",
      "[-0.60644001 -0.51204997  0.0064921  -0.29194    -0.56515002]\n"
     ]
    }
   ],
   "source": [
    "#Spacy\n",
    "king_vector = generate_spacy_vector_embedding(\"King\")\n",
    "queen_vector = generate_spacy_vector_embedding(\"Queen\")\n",
    "man_vector = generate_spacy_vector_embedding(\"man\")\n",
    "woman_vector = generate_spacy_vector_embedding(\"woman\")\n",
    "print(f\"This embedding has {len(king_vector)} dimensions\")\n",
    "print(king_vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f48a589-3c56-4a5a-80a6-74a372c7a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity distance between Spacey calculated_queen - Queen: 0.4812\n",
      "Cosine Similarity of Spacey King to Queen: 0.3825\n"
     ]
    }
   ],
   "source": [
    "calculated_queen_vector = king_vector - man_vector + woman_vector\n",
    "\n",
    "similarity = cosine_similarity(calculated_queen_vector, queen_vector)\n",
    "print(f\"Cosine Similarity distance between Spacey calculated_queen - Queen: {similarity:.4f}\")\n",
    "\n",
    "similarity = cosine_similarity(king_vector, queen_vector)\n",
    "print(f\"Cosine Similarity of Spacey King to Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55107547-7270-4c72-8a1f-1afc4f6d7d40",
   "metadata": {},
   "source": [
    "Let's examine other phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4f8306f-e557-4fe6-af63-8f73c6b4c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of cat to book using Titan: 0.3514\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(generate_titan_vector_embedding(\"cat\"), generate_titan_vector_embedding(\"book\"))\n",
    "print(f\"Cosine Similarity of cat to book using Titan: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9acb5e7e-726f-4054-bf8e-760e4b9fa440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of cat to book using Cohere: 0.5562\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(generate_cohere_vector_embedding(\"cat\"), generate_cohere_vector_embedding(\"book\"))\n",
    "print(f\"Cosine Similarity of cat to book using Cohere: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4362cdf3-c640-4628-82cb-688c4d631998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of cat to book using Spacey: 0.0693\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(generate_spacy_vector_embedding(\"cat\"), generate_spacy_vector_embedding(\"book\"))\n",
    "print(f\"Cosine Similarity of cat to book using Spacey: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aaa723-3154-4336-8d3e-35d83d117d7b",
   "metadata": {},
   "source": [
    "Now let's look at a larger sentences and see how larger models with more complexity handle the same task Here are 2 sentences that semantically similar but use different words and phrasing.\n",
    "\n",
    "The majestic, towering skyscrapers, their gleaming windows reflecting the golden rays of the setting sun, stood as a testament to human ingenuity and the indomitable spirit of progress, while the bustling streets below teemed with life as people from all walks of life hurried to their destinations, their faces a mix of determination and weariness, yet each individual contributing to the vibrant tapestry of the city's existence.\n",
    "\n",
    "The awe-inspiring, colossal high-rises, their polished glass facades mirroring the warm, amber glow of the fading daylight, served as a powerful symbol of human innovation and the unyielding drive for advancement, as the lively thoroughfares beneath pulsed with energy, filled with individuals from diverse backgrounds rushing to their intended locations, their expressions an amalgamation of resolve and fatigue, yet all playing a vital role in the dynamic, intricate mosaic that shaped the city's vibrant identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb721f75-5bae-4da9-8344-e5811c2f31e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of S1 to S2 using Spacey: 0.9637\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The majestic, towering skyscrapers, their gleaming windows reflecting the golden rays of the setting sun, stood as a testament to human ingenuity and the indomitable spirit of progress, while the bustling streets below teemed with life as people from all walks of life hurried to their destinations, their faces a mix of determination and weariness, yet each individual contributing to the vibrant tapestry of the city's existence.\"\n",
    "sentence2 = \"The awe-inspiring, colossal high-rises, their polished glass facades mirroring the warm, amber glow of the fading daylight, served as a powerful symbol of human innovation and the unyielding drive for advancement, as the lively thoroughfares beneath pulsed with energy, filled with individuals from diverse backgrounds rushing to their intended locations, their expressions an amalgamation of resolve and fatigue, yet all playing a vital role in the dynamic, intricate mosaic that shaped the city's vibrant identity.\"\n",
    "similarity = cosine_similarity(generate_spacy_vector_embedding(sentence1), generate_spacy_vector_embedding(sentence2))\n",
    "print(f\"Cosine Similarity of S1 to S2 using Spacey: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86acf2c4-bc8a-49d4-9ea4-5488a96c93aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of S1 to S2 using Titan: 0.8726\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The majestic, towering skyscrapers, their gleaming windows reflecting the golden rays of the setting sun, stood as a testament to human ingenuity and the indomitable spirit of progress, while the bustling streets below teemed with life as people from all walks of life hurried to their destinations, their faces a mix of determination and weariness, yet each individual contributing to the vibrant tapestry of the city's existence.\"\n",
    "sentence2 = \"The awe-inspiring, colossal high-rises, their polished glass facades mirroring the warm, amber glow of the fading daylight, served as a powerful symbol of human innovation and the unyielding drive for advancement, as the lively thoroughfares beneath pulsed with energy, filled with individuals from diverse backgrounds rushing to their intended locations, their expressions an amalgamation of resolve and fatigue, yet all playing a vital role in the dynamic, intricate mosaic that shaped the city's vibrant identity.\"\n",
    "similarity = cosine_similarity(generate_titan_vector_embedding(sentence1), generate_titan_vector_embedding(sentence2))\n",
    "print(f\"Cosine Similarity of S1 to S2 using Titan: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4131bfda-1db4-4b29-b184-0c0bdc03779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of S1 to S2 using Cohere: 0.8160\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The majestic, towering skyscrapers, their gleaming windows reflecting the golden rays of the setting sun, stood as a testament to human ingenuity and the indomitable spirit of progress, while the bustling streets below teemed with life as people from all walks of life hurried to their destinations, their faces a mix of determination and weariness, yet each individual contributing to the vibrant tapestry of the city's existence.\"\n",
    "sentence2 = \"The awe-inspiring, colossal high-rises, their polished glass facades mirroring the warm, amber glow of the fading daylight, served as a powerful symbol of human innovation and the unyielding drive for advancement, as the lively thoroughfares beneath pulsed with energy, filled with individuals from diverse backgrounds rushing to their intended locations, their expressions an amalgamation of resolve and fatigue, yet all playing a vital role in the dynamic, intricate mosaic that shaped the city's vibrant identity.\"\n",
    "similarity = cosine_similarity(generate_cohere_vector_embedding(sentence1), generate_cohere_vector_embedding(sentence2))\n",
    "print(f\"Cosine Similarity of S1 to S2 using Cohere: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077766d-8d7b-47a1-a816-6c9b2906ea1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsb570env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
