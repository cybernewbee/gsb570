{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c42ed4-efdc-43e5-ada7-159a287c37a6",
   "metadata": {},
   "source": [
    "### Breaking data down into useful chunks\n",
    "![Chunking](./images/chunks.jpg)\n",
    "<p>In order to allow LLMs to intelligently interact with content outside the large corpus of text that LLMS have been trained on, we need to orgainze that data into chunks that we will retrieve based on relevancy.  The first step in this process is to break up our provided knowledge into managable chunks.  Recall LLMs have a limited context window to consume data in so it's up to us to parse it into meaningfull bits.\n",
    "</p>\n",
    "<p>There are many strategies we can use to depending on the type of data we are ingesting. Here are a few we will review</p>\n",
    "\n",
    "    1. Fixed-Size Chunking \n",
    "    2. Sentence-Level Chunking\n",
    "    3. Semantic Based \n",
    "\n",
    "![Chunking](./images/top3.jpg)\n",
    "Image taken from [SOURCE](https://www.nb-data.com/p/9-chunking-strategis-to-improve-rag)\n",
    "\n",
    "| Use Case                                    | Strategy                                           |\n",
    "|:--------------------------------------------|:---------------------------------------------------|\n",
    "| Small documents / simple use case\t      | Fixed-size chunking                        |\n",
    "| Frequently Asked Questions           | Sentence level chunking              |\n",
    "| High semantic fidelity needed RAG             | Semantic chunking                      |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457cf99d-1cec-40da-bd82-8aa8e36c51d5",
   "metadata": {},
   "source": [
    "### Simple\n",
    "Let's review a simple fixed size approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a8738-f81b-490e-a802-6aedb35265c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd2ac6-ed0d-494a-86e0-84e812b85542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def fixed_size_chunking(text, chunk_size, overlap):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += chunk_size - overlap  # move start forward with overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Note you may need to run this in a terminal window if you ge\n",
    "# python -m spacy download en_core_web_md\n",
    "# now restart your kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c614d6-9d3f-417c-b631-d521e3ab965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100\n",
    "overlap_size = 20\n",
    "\n",
    "# open file and read text from file\n",
    "# Example usage\n",
    "file_path = \"./data/register-for-classes.txt\"\n",
    "file_content = read_file(file_path)\n",
    "\n",
    "if file_content is None:\n",
    "    print(\"Unable to read data from file: \", file_path)\n",
    "\n",
    "# Generate chunks\n",
    "chunks = fixed_size_chunking(file_content, chunk_size, overlap_size)\n",
    "\n",
    "# Display results\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i + 1} ---\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83654610-f441-41a8-9a3e-125ab7a8b3ee",
   "metadata": {},
   "source": [
    "### Sentence\n",
    "Now let's get a little more sophisticated and use sentance level encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f5355-a126-4601-b932-db61be18c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_chunking(text, sentences_per_chunk):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = \" \".join(sentences[i:i + sentences_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375b84b-fd2f-4e7c-97b6-27fa51f0d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_per_chunk = 1\n",
    "\n",
    "# open file and read text from file\n",
    "# Example usage\n",
    "file_path = \"./data/declaration-of-indep.txt\"\n",
    "file_content = read_file(file_path)\n",
    "\n",
    "if file_content is None:\n",
    "    print(\"Unable to read data from file: \", file_path)\n",
    "\n",
    "# Generate chunks\n",
    "chunks = sentence_chunking(file_content, sentences_per_chunk)\n",
    "\n",
    "# Display results\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i + 1} ---\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed59ad3-b820-427b-9def-abdfc32eb752",
   "metadata": {},
   "source": [
    "### Semantic\n",
    "Let's explore semantic based chunkng approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373671b-cfb7-4110-b748-c83c4b6c6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_embedding_chunk(text, threshold):\n",
    "    \"\"\"\n",
    "    Splits text into semantic chunks using sentence embeddings.\n",
    "    Uses spaCy for sentence segmentation and SentenceTransformer for generating embeddings.\n",
    "\n",
    "    :param text: The full text to chunk.\n",
    "    :param threshold: Cosine similarity threshold for adding a sentence to the current chunk.\n",
    "    :return: A list of semantic chunks (each as a string).\n",
    "    \"\"\"\n",
    "    # Sentence segmentation\n",
    "    #doc = nlp(text)\n",
    "    #sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    sentences = fixed_size_chunking(text, 100, 10)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_embedding = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Generate embedding for the current sentence\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "        # If starting a new chunk, initialize it with the current sentence\n",
    "        if current_chunk_embedding is None:\n",
    "            current_chunk_sentences = [sentence]\n",
    "            current_chunk_embedding = sentence_embedding\n",
    "        else:\n",
    "            # Compute cosine similarity between current sentence and the chunk embedding\n",
    "            sim_score = util.cos_sim(sentence_embedding, current_chunk_embedding)\n",
    "            if sim_score.item() >= threshold:\n",
    "                # Add sentence to the current chunk and update the chunk's average embedding\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                num_sents = len(current_chunk_sentences)\n",
    "                current_chunk_embedding = ((current_chunk_embedding * (num_sents - 1)) + sentence_embedding) / num_sents\n",
    "            else:\n",
    "                # Finalize the current chunk and start a new one\n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_chunk_embedding = sentence_embedding\n",
    "\n",
    "    # Append the final chunk if it exists\n",
    "    if current_chunk_sentences:\n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664bc92-7b3a-4d8c-a776-2914b6e134a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f14a51-07e5-4c9e-a430-c1627010f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "file_path = \"./data/home-care.txt\"\n",
    "home_care_content = read_file(file_path)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "semantic_chunks = semantic_embedding_chunk(home_care_content, threshold=0.45)\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cb978-f8e4-4c07-8ca6-3730fc66f0c1",
   "metadata": {},
   "source": [
    "Spacy is a powerful NLP library that can be used for lots of other parsing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df685866-52ac-4635-9340-cf2eec7f2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nouns(text):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(text)\n",
    "    for noun in doc.noun_chunks:\n",
    "        print (noun)\n",
    "        \n",
    "def find_entites(text):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(text)\n",
    "    for entity in doc.ents:\n",
    "        print (entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36836cab-6d4a-49a5-b4e8-7c9a3f336b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunking(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73808998-6c6b-42b7-ae60-3bd385a8e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_entites(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a7a03c-8214-453d-83fd-f219a73d5eb3",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Those basic chunking strategies should cover most of your needs but spending time up front on the right chunking statefgy will really help improve the quality of you retrieval.\n",
    "\n",
    "#### Assignment\n",
    "Go find some data that may be useful to you in your project and determine a chunking strategy that might work for that content.  Use what you think is the best method to put the dataset into chunks.  Turn in your source data and chunking code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
