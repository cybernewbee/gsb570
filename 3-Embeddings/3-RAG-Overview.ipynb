{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf86a47-b70a-498a-8dbc-de917d47fa84",
   "metadata": {},
   "source": [
    "### Retrival Augmentented Generation\n",
    "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a4fed-79a7-4616-96b5-813c6f5bcc9a",
   "metadata": {},
   "source": [
    "Standard imports for the libraires we will be using in this notebook.  Try to keep your imports in the first cell so this can this code can more easliy be converted into a python program later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fac6d6-0bd0-4638-8f2a-ffc1817226e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_cohere -q\n",
    "#%pip install spacy -q\n",
    "%pip install psycopg2-binary -q\n",
    "#%pip install python-dotenv -q\n",
    "#ignore error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513731f-ab9f-4e74-9249-e8ff08dfd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import traceback\n",
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "import dbconnection\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create the AWS client for the Bedrock runtime with boto3\n",
    "aws_client = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14383a5-eec2-40e7-9c61-ccadb437d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_string_size(x, max_chars=2048):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(x, str):\n",
    "        return x[:max_chars]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d401dce-efa2-4cf7-89e8-ce9fcae7e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(value):\n",
    "    value_str = str(value)\n",
    "    cleaned_value = ''.join(char for char in value_str if char.isalnum() or char.isspace())\n",
    "    return cleaned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904a743-b176-459a-b455-235deffffbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_values(list_stuff: list, num_items: int) -> None:\n",
    "    i=0\n",
    "    for item in list_stuff:\n",
    "        i=i+1\n",
    "        if i>num_items:\n",
    "            return None\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35157a12-af69-4d79-8a9f-d7229eef4bec",
   "metadata": {},
   "source": [
    "#### Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0667bf3-01ef-4493-8cbc-744413c0d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send in an array size of one and only return the 0th element\n",
    "def generate_cohere_vector_embedding(text_data):\n",
    "    input_type = \"clustering\"\n",
    "    truncate = \"NONE\" # optional\n",
    "    model_id = \"cohere.embed-english-v3\" # or \"cohere.embed-multilingual-v3\"\n",
    "    trunc_data = limit_string_size(text_data)\n",
    "    # Create the JSON payload for the request\n",
    "    json_params = {\n",
    "            'texts': [trunc_data],\n",
    "            'truncate': truncate, \n",
    "            \"input_type\": input_type\n",
    "        }\n",
    "    json_body = json.dumps(json_params)\n",
    "    params = {'body': json_body, 'modelId': model_id,}\n",
    "    \n",
    "    # Invoke the model and print the response\n",
    "    result = aws_client.invoke_model(**params)\n",
    "    response = json.loads(result['body'].read().decode())\n",
    "    return(np.array(response['embeddings'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0f33d-429d-4636-9082-d7bbb9850790",
   "metadata": {},
   "source": [
    "#### Amazon Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794765f-0b32-451e-a145-6a7b4ce785e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a dense vector using Amazon Titan with LangChain\n",
    "def generate_titan_vector_embedding(text):\n",
    "    #create an Amazon Titan Text Embeddings client\n",
    "    embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\") \n",
    "\n",
    "    #Invoke the model\n",
    "    embedding = embeddings_client.embed_query(text)\n",
    "    return(np.array(embedding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff40e7e-c880-4dd1-b7ad-70f8a12efe2a",
   "metadata": {},
   "source": [
    "This is the mathmatical formula to calcuate cosine similarity between 2 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8367a7-c786-41fc-ad05-d53b5633d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705047d-8d00-440c-a3e8-05b3efa89edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a dense vector using Amazon Titan without using a np.array as a return value\n",
    "def generate_vector_embedding(text):\n",
    "    #create an Amazon Titan Text Embeddings client\n",
    "    embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\") \n",
    "\n",
    "    #Invoke the model\n",
    "    embedding = embeddings_client.embed_query(text)\n",
    "    #Note pgvector does not want a np.array as out manual method\n",
    "    return(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7230a6-86c4-4950-a793-6bae2bb4925d",
   "metadata": {},
   "source": [
    "#### Storage Data for Retrieval\n",
    " In order to do semantic search and retrieve relevant content we need to store that content for later use.  We can store the embedding in several different persistence technologies.  To start simply let's store the data in memory using pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b1080-7652-4333-ad30-ad57d36d5868",
   "metadata": {},
   "source": [
    "Using Gaggle I looked for an interesting dataset that had data that I wanted to use to answer specific questions that is likely not in the large corpus of trained data. Let's put the data into a pandas dataframe and examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11db83-cffa-4ebc-8f41-5392bb92cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean abstract text\n",
    "df = pd.read_csv('data/latest_research_articles.csv')\n",
    "df['abstract'] = df['abstract'].apply(clean_value)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a5b32-5c78-48ea-b4cc-26ed804bc043",
   "metadata": {},
   "source": [
    "#### Store embedded values in memory\n",
    "Now let's create a new column that represents the article abstract as a vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43021e17-eede-416e-8b84-aa65b30e8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using Titan\n",
    "# Add a new column 'embedded_abstract' by applying the function to an existing column\n",
    "# This step takes a while so I did it for you and saved the output as pickle\n",
    "#dft = df.copy()\n",
    "#dft['embedded_abstract'] = dft['abstract'].apply(generate_titan_vector_embedding)\n",
    "#df.to_pickle('data/embedded_df.pkl')\n",
    "\n",
    "# Easier way\n",
    "dft = pd.read_pickle('data/embedded_df.pkl')\n",
    "dft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08975b47-f547-4930-a7e4-42876c00e7e9",
   "metadata": {},
   "source": [
    "### Retrieval from embedded sources\n",
    "Now that we have a dataframe with embedded content of interest, we can use semantic similarity to retrieve the right data to feed to an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b25889-1d6a-4c2f-bd19-5d117cff23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's setup a query that a user might ask\n",
    "query = \"What is the latest research for broken ribs in children\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0949d-dad2-4fa3-b6cc-a11c5a61fc09",
   "metadata": {},
   "source": [
    "#### Let's send a query in and see hoe the vector embeddings compare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d48ef-2c13-4f4f-9389-76b60de4ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's search our records for a good semantic search\n",
    "query_vector = generate_titan_vector_embedding(query)\n",
    "\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dft.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, value)\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the sorted data\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    article_title = dft.iloc[item[0]]['title']\n",
    "    print(f\"Abstract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858166f7-846f-411b-8cd6-7f147ae62128",
   "metadata": {},
   "source": [
    "#### Cohere Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3af81-5d8c-4fe6-95d1-6ac172969a44",
   "metadata": {},
   "source": [
    "Each of these models also have a [max input tokens limit](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-embed.html) and that determines the largest number of characters of text we can embed.\n",
    "\n",
    "[https://cohere.com/blog/introducing-embed-v3](https://cohere.com/blog/introducing-embed-v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2815ed-ffe2-469a-96aa-b73e9d642a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using Cohere\n",
    "# Add a new column 'embedded_abstract' by applying the function to an existing column\n",
    "# This step takes a while so I did it for you and saved the output as pickle\n",
    "#df['embedded_abstract'] = df['abstract'].apply(generate_cohere_vector_embedding)\n",
    "#df.to_pickle('data/cohere_embedded.pkl')\n",
    "\n",
    "# This step takes a while so I did it for you and saved the output as pickle\n",
    "dfc = pd.read_pickle('data/cohere_embedded.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's search our records for a good semantic search\n",
    "query_vector = generate_cohere_vector_embedding(query)\n",
    "\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dfc.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, value)\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the sorted data\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    article_title = dfc.iloc[item[0]]['title']\n",
    "    print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f08de8-5b56-42fe-9337-7690c819c968",
   "metadata": {},
   "source": [
    "### Vector Database for Larger Datasets\n",
    "For our first example we used a local dataframe to store the contents of our raw text and embeddings, then manually calculated similarity between embeddings.  There are other tools that are better suited for larger datasets with embeddings. Vector databases!  We will explore pgvector running on a Postgresql database engine.\n",
    "\n",
    "### Switch from a dataframe to a database\n",
    "Just like we used a dataframe earlier, now we will use a database instead.  This will allow us to store many more records and persist them to disk so we don't just keep them in memory.  Databases are much more efficient at storing large sets of data and being able to calulate cosine similarity more efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c000a-330a-4880-be32-39cf39b5e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's review our IP address in case we haev any issues accessing the DB\n",
    "from requests import get\n",
    "\n",
    "ip = get('https://api.ipify.org').content.decode('utf8')\n",
    "print('My public IP address is: {}'.format(ip))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3aa72-0500-44c7-a78c-a3e798e935b4",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "Configure this item for your data.  We are sharing a Vector DB and this value will segment your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd5910-f200-43dd-ae84-a374ca0e428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your username so yoru database entry will be unique to you\n",
    "#MY_USERNAME = \"dkraker@calpoly.edu\"\n",
    "MY_USERNAME = \"YOUR_USERNAME_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5edab-c516-4915-a4da-2812b66b18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_record_into_db(table_name, record, conn):\n",
    "\n",
    "    # Dynamically generates and executes an INSERT SQL statement for PostgreSQL, handling\n",
    "    # special data types like datetime objects and arrays directly.\n",
    "    \n",
    "    # Args:\n",
    "    # - table_name (str): The name of the table into which the record will be inserted.\n",
    "    # - data (dict): A dictionary representing the record to be inserted, where keys are column names\n",
    "    #                  and values can include native PostgreSQL types like datetime and arrays.\n",
    "    # - conn (psycopg2.connection): A psycopg2 connection object.\n",
    "    \n",
    "    # Generate column names and placeholders\n",
    "    columns = ', '.join(record.keys())\n",
    "    placeholders = ', '.join(['%s'] * len(record))  # PostgreSQL uses %s as placeholder\n",
    "\n",
    "    # Create the INSERT INTO statement\n",
    "    sql = f'INSERT INTO {table_name} ({columns}) VALUES ({placeholders})'\n",
    "    # print(\"columns=\", columns)\n",
    "    # print(\"placeholders=\", placeholders)\n",
    "    # print(sql)\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute(sql, tuple(record.values()))\n",
    "        conn.commit()\n",
    "        print(\"Record inserted successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"An add record error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6155de1-8cd4-43e9-8fb8-8a872bc9d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a staff report for the city of SLO that contains 10 pages of data.  We will chunk the report into 1024 character chunks\n",
    "# with a 256 character overlap.  We will insert each chunk into the DB with embeddings for similarity search.\n",
    "with open(\"data/staff-report.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,  # Adjust the chunk size as needed\n",
    "    chunk_overlap=256,  # Adjust the overlap between chunks as needed\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Now for each chunk open a connection to our DB and insert a record into our generic schema\n",
    "\n",
    "conn = dbconnection.open_connection_to_db()\n",
    "try:\n",
    "    # create a dataframe with new chunk raw text\n",
    "    for chunk in chunks:\n",
    "        data_record = {}\n",
    "        v_embed = generate_vector_embedding(chunk)\n",
    "        data_record[\"username\"] = MY_USERNAME\n",
    "        data_record[\"textattribute1\"] = chunk\n",
    "        data_record[\"textattribute2\"] = \"\"\n",
    "        data_record[\"textattribute3\"] = \"\"\n",
    "        data_record[\"textattribute4\"] = \"\"\n",
    "        data_record[\"textattribute5\"] = \"\"\n",
    "        data_record[\"textembedding1\"] = v_embed\n",
    "        insert_record_into_db(\"rag\", data_record, conn)        \n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while inserting records: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060d546-370f-481c-934f-c02274c464f3",
   "metadata": {},
   "source": [
    "#### Review our data in the DB\n",
    "Now that we've chunked and inserted our data let's do a quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d12f5c-bd8f-4ba9-b43f-f8b3e13d4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_my_data(conn):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # SQL statement to delete rows where username is 'bob'\n",
    "        sql = f\"SELECT textattribute1, textattribute2, textattribute3, textattribute4, textattribute5 FROM public.rag WHERE username = '{MY_USERNAME}'\"\n",
    "    \n",
    "        # Execute the SQL statement\n",
    "        cur.execute(sql)\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        # grab the cosine scores so we can compute Z score for narrow article selection\n",
    "        for row in rows:\n",
    "            print(row[0], row[1], row[2], row[3], row[4])\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa91d8-f52f-4516-9955-4b7bd0f496d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = dbconnection.open_connection_to_db()\n",
    "view_my_data(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5041bc9-398d-4668-95ee-5f32d895a4a1",
   "metadata": {},
   "source": [
    "#### Now that our data is in the DB we are ready to retrieve\n",
    "Something we need to consider during retrieval how many records do we think are important.  We have a few choices\n",
    "- Top N results\n",
    "- Threshhold\n",
    "- Fall off values<br>\n",
    "Let's explore Z-score as a first approach<br>\n",
    "![Z-Score Example](./images/zscore-dist.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd34898-ba5f-4b26-8034-603b6da8925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_zscores(cosine_scores):\n",
    "    zscores = []\n",
    "    # Calculate the mean of the sample points\n",
    "    mean = np.mean(cosine_scores)\n",
    "    # Calculate the standard deviation of the sample points\n",
    "    std_deviation = np.std(cosine_scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "    # Calculate the z-scores for each sample point\n",
    "    z_scores = [(x - mean) / std_deviation for x in cosine_scores]\n",
    "\n",
    "    return z_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae32b2-6bfa-4e42-8b4b-d69de5e96e4f",
   "metadata": {},
   "source": [
    "Now let's use the database as a way to find the best match. Notice the 2 different commented SQL syntax differences.  We can easily compute Euclidean distance as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a42cf2-453b-477d-84c9-1c2913384790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_similarity_search_pgvector(question, embedded_text, conn):\n",
    "    # Cosine similarity\n",
    "    #1-(textembedding1 <=> ('{embedded_text}')) as cosine_similar  \\\n",
    "    # Euclidean distance\n",
    "    #textembedding1 <-> ('{embedded_text}') as euclidean_distance  \\\n",
    "    sql = f\"SELECT textattribute1, textattribute2, textattribute3, textattribute4, textattribute5, \\\n",
    "                1-(textembedding1 <=> ('{embedded_text}')) as cosine_similar  \\\n",
    "                FROM public.rag \\\n",
    "                WHERE username = '{MY_USERNAME}' \\\n",
    "                ORDER BY cosine_similar DESC \\\n",
    "                LIMIT 50\"\n",
    "    #print(sql)\n",
    "    cosine_scores = []\n",
    "    try:\n",
    "        article_text = \"\"\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        # grab the cosine scores so we can compute Z score for narrow article selection\n",
    "        # need all scores so we can calc Z\n",
    "        for row in rows:\n",
    "            #print(row[5])\n",
    "            cosine_scores.append(row[5])\n",
    "            \n",
    "        z_scores = calculate_zscores(cosine_scores)\n",
    "        answer = \"Unknown\"\n",
    "        article_text = \"\"\n",
    "        zscore_index = 0\n",
    "        first_z_score = z_scores[0]\n",
    "        for row in rows:\n",
    "            if(first_z_score/2)<z_scores[zscore_index]:\n",
    "                print(f\"Using chunk with a cosine match of: {row[5]} and Z-score of: {z_scores[zscore_index]}\")\n",
    "                article_text = article_text + row[0] + \"\\n\"\n",
    "            zscore_index += 1\n",
    "        #print(article_text)\n",
    "        answer = best_answer(article_text, question)\n",
    "        #Close cursor and connection\n",
    "        cur.close()\n",
    "        return answer\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cceba1-c9fc-4deb-b06e-9eb0033920af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's write a function that will take retrieval data and use an LLM to generate a good answer\n",
    "def best_answer(data, question):\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    model_kwargs =  { \n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "    }\n",
    "\n",
    "    model = BedrockChat(\n",
    "        client=aws_client,\n",
    "        model_id=model_id,\n",
    "        model_kwargs=model_kwargs,\n",
    "    )\n",
    "\n",
    "    human_prompt = \"You are to answer the question using the data in the following information.  Do not make up your answer, only use \\\n",
    "                    supporting data from the article, If you don't have enough data simply respond, I don't have enough information to answer that question. \\\n",
    "                    given the following article data {data} can you please give a concise answer to the following question. {question}\"\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful assistant that can answer quesitons based on news articles you have been given.\"),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    "    try:\n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "        chain = prompt | model | StrOutputParser()\n",
    "\n",
    "        # Chain Invoke\n",
    "        \n",
    "    \n",
    "        # Send the message content to Claude using Bedrock and get the response\n",
    "        start_time = time.time()  # Start timing\n",
    "        # Call Bedrock\n",
    "        response = chain.invoke({\"data\": data,\"question\": question})\n",
    "        end_time = time.time()  # End timing\n",
    "        #print(\"Claude call took :\", end_time - start_time)  # Calculate execution time\n",
    "\n",
    "        return(response)\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = traceback.sys.exc_info()\n",
    "        line_number = exc_traceback.tb_lineno\n",
    "\n",
    "        return f\"ERROR generating good answer: {exc_type}{exc_value}{exc_traceback} on {line_number}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7129e31-471a-454b-b05c-74b522d93205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge_my_data(conn):\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # SQL statement to delete rows where username is 'bob'\n",
    "        sql = f\"DELETE FROM public.rag WHERE username = '{MY_USERNAME}'\"\n",
    "    \n",
    "        # Execute the SQL statement\n",
    "        cur.execute(sql)\n",
    "    \n",
    "        # Commit the changes to the database\n",
    "        conn.commit()\n",
    "    \n",
    "        # Get the number of affected rows\n",
    "        deleted_rows = cur.rowcount\n",
    "        print(f\"{deleted_rows} row(s) deleted.\")\n",
    "\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78ac33-7118-4a62-adaa-9a1566c7cd0d",
   "metadata": {},
   "source": [
    "#### It's magic time!\n",
    "<p>Ok let's review what we've done to get here</p>\n",
    "1. Ingested chunks of data from a large text file<br>\n",
    "2. Embedded each chunk into a vector DB<br>\n",
    "3. Wrote a function to determine the right number of retrievals to consider using Z-score<br>\n",
    "4. Wrote a function that asks the LLM to find us the best answer from retrieved chunks<br>\n",
    "<br>\n",
    "<p>Let's send in a question and find the best chunks to answer it!!!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e6811-c304-4489-b2d4-72d69da3d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"What is the city recommending?\"\n",
    "query = \"What will it cost to replace the mixer gearbox?\"\n",
    "\n",
    "query_vector = generate_vector_embedding(query)\n",
    "\n",
    "conn = dbconnection.open_connection_to_db()\n",
    "\n",
    "run_similarity_search_pgvector(query, query_vector, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6c4e8-e7f1-40a0-b34a-84e5d67a86e5",
   "metadata": {},
   "source": [
    "### WARNING this will DELETE all your vector DB data\n",
    "Use this if you want to removed your embedded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378318f-e593-4203-b9bd-e9d9027f9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = dbconnection.open_connection_to_db()\n",
    "purge_my_data(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43370a1b-0050-4b29-b988-ed43c4bb1357",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "Find a interesting set of information data that might be a candidate for your project\n",
    "Ingest this data into a data store for later retrieval. Choose either a dataframe or use the class vector DB\n",
    "Choose a retrieval method to respond to input queries and using either an approach demonstrated in class or one of your own that reduces the retrieval context length\n",
    "Use an LLM of your choosing to provide a concise answer to the question asked. Ensure the information provided can be referenced in your original source data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b8284-f3a3-408f-8626-a3086a2f32ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
